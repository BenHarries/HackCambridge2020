<!DOCTYPE html>
<html>
<head>
    <title>Speech Sample</title>
    <meta charset="utf-8" />
</head>
<body style="font-family:'Helvetica Neue',Helvetica,Arial,sans-serif; font-size:13px;">
<div id="warning">
    <h1 style="font-weight:500;">Speech Recognition Speech SDK not found (microsoft.cognitiveservices.speech.sdk.bundle.js missing).</h1>
</div>
<div id="content" style="display:none">
    <table>

        <tr>
            <td align="right"></td>
            <td>
                <button id="speechsdkStartContinuousRecognition">startContinuousRecognitionAsync()</button>
                <button id="speechsdkStopContinuousRecognition" disabled="disabled">STOP stopContinuousRecognitionAsync()</button>
            </td>
        </tr>

            <td align="right">Phrase List Values:</td>
            <td>
                <input id="phrases" type="text" size="60" value="">(';' separated list)
            </td>
        </tr>
        <tr>
            <td align="right">Results:</td>
            <td align="left">
                <textarea id="phraseDiv" style="display: inline-block;width:200px;height:200px"></textarea>
            </td>
        </tr>
        <tr>
            <td align="right"></td>
            <td align="left"><textarea id="statusDiv" style="display: inline-block;width:200px;height:200px;overflow: scroll;white-space: nowrap;"></textarea></td>
        </tr>
    </table>
</div>

<!-- Speech SDK REFERENCE -->
<script src="../microsoft.cognitiveservices.speech.sdk.bundle.js"></script>

<!-- Speech Speech SDK Authorization token -->
<script>
  // Note: Replace the URL with a valid endpoint to retrieve
  //       authorization tokens for your subscription.

  // An authorization token is a more secure method to authenticate for a browser deployment as
  // it allows the subscription keys to be kept secure on a server and a 10 minute use token to be
  // handed out to clients from an endpoint that can be protected from unauthorized access.

</script>

<!-- Speech SDK USAGE -->
<script>
  // On document load resolve the Speech SDK dependency
  function Initialize(onComplete) {
    if (!!window.SpeechSDK) {
      document.getElementById('content').style.display = 'block';
      document.getElementById('warning').style.display = 'none';
      onComplete(window.SpeechSDK);
    }
  }

  function compareText(readPhrase) {
    var samplePhrase = "I like to eat chips";
    samplePhrase.replace(/[.,\/#!$%\^&\*;:{}=\-_`~()]/g,"");
    samplePhrase.replace(/\s{2,}/g," ");

    console.log(str.split(samplePhrase, " "));


  }
</script>

<!-- Browser Hooks -->
<script>
  var phraseDiv, statusDiv;
  var phrases;
  var SpeechSDK;
  var recognizer;

  var reco;
  var sdkStartContinousRecognitionBtn, sdkStopContinousRecognitionBtn;
  var voiceOutput;

  var soundContext = undefined;
  try {
    var AudioContext = window.AudioContext // our preferred impl
      || window.webkitAudioContext       // fallback, mostly when on Safari
      || false;                          // could not find.

    if (AudioContext) {
      soundContext = new AudioContext();
    } else {
      alert("Audio context not supported");
    }
  }
  catch (e) {
    window.console.log("no sound context found, no audio output. " + e);
  }

  document.addEventListener("DOMContentLoaded", function () {
    createBtn = document.getElementById("createBtn");
    sdkStartContinousRecognitionBtn = document.getElementById("speechsdkStartContinuousRecognition");
    sdkStopContinousRecognitionBtn = document.getElementById("speechsdkStopContinuousRecognition");
    phraseDiv = document.getElementById("phraseDiv");
    statusDiv = document.getElementById("statusDiv");
    phrases = document.getElementById("phrases");
    voiceOutput = document.getElementById("voiceOutput");

    // Starts continuous speech recognition.
    sdkStartContinousRecognitionBtn.addEventListener("click", function () {
      phraseDiv.innerHTML = "";
      statusDiv.innerHTML = "";
      var lastRecognized = "";

      // If an audio file was specified, use it. Else use the microphone.
      // Depending on browser security settings, the user may be prompted to allow microphone use. Using continuous recognition allows multiple
      // phrases to be recognized from a single use authorization.
      var audioConfig = SpeechSDK.AudioConfig.fromDefaultMicrophoneInput();

      var speechConfig;
      speechConfig = SpeechSDK.SpeechConfig.fromSubscription("f9bef20c5e064a65b33c3467e56f8e8b", "westus");

      speechConfig.speechRecognitionLanguage = 'en-US';
      reco = new SpeechSDK.SpeechRecognizer(speechConfig, audioConfig);

      // Before beginning speech recognition, setup the callbacks to be invoked when an event occurs.

      // The event recognizing signals that an intermediate recognition result is received.
      // You will receive one or more recognizing events as a speech phrase is recognized, with each containing
      // more recognized speech. The event will contain the text for the recognition since the last phrase was recognized.
      reco.recognizing = function (s, e) {
        window.console.log(e);
        statusDiv.innerHTML += "(recognizing) Reason: " + SpeechSDK.ResultReason[e.result.reason] + " Text: " + e.result.text + "\r\n";
        phraseDiv.innerHTML = lastRecognized + e.result.text;
      };

      // The event recognized signals that a final recognition result is received.
      // This is the final event that a phrase has been recognized.
      // For continuous recognition, you will get one recognized event for each phrase recognized.
      reco.recognized = function (s, e) {
        window.console.log(e);

        // Indicates that recognizable speech was not detected, and that recognition is done.
        if (e.result.reason === SpeechSDK.ResultReason.NoMatch) {
          var noMatchDetail = SpeechSDK.NoMatchDetails.fromResult(e.result);
          statusDiv.innerHTML += "(recognized)  Reason: " + SpeechSDK.ResultReason[e.result.reason] + " NoMatchReason: " + SpeechSDK.NoMatchReason[noMatchDetail.reason] + "\r\n";
        } else {
          statusDiv.innerHTML += "(recognized)  Reason: " + SpeechSDK.ResultReason[e.result.reason] + " Text: " + e.result.text + "\r\n";
          console.log('recognised');
          compareText(e.result.text);
        }

        lastRecognized += e.result.text + "\r\n";
        phraseDiv.innerHTML = lastRecognized;
      };

      // The event signals that the service has stopped processing speech.
      // https://docs.microsoft.com/javascript/api/microsoft-cognitiveservices-speech-sdk/speechrecognitioncanceledeventargs?view=azure-node-latest
      // This can happen for two broad classes of reasons.
      // 1. An error is encountered.
      //    In this case the .errorDetails property will contain a textual representation of the error.
      // 2. No additional audio is available.
      //    Caused by the input stream being closed or reaching the end of an audio file.
      reco.canceled = function (s, e) {
        window.console.log(e);

        statusDiv.innerHTML += "(cancel) Reason: " + SpeechSDK.CancellationReason[e.reason];
        if (e.reason === SpeechSDK.CancellationReason.Error) {
          statusDiv.innerHTML += ": " + e.errorDetails;
        }
        statusDiv.innerHTML += "\r\n";
      };

      // Signals that a new session has started with the speech service
      reco.sessionStarted = function (s, e) {
        window.console.log(e);
        statusDiv.innerHTML += "(sessionStarted) SessionId: " + e.sessionId + "\r\n";
      };

      // Signals the end of a session with the speech service.
      reco.sessionStopped = function (s, e) {
        window.console.log(e);
        statusDiv.innerHTML += "(sessionStopped) SessionId: " + e.sessionId + "\r\n";
        sdkStartContinousRecognitionBtn.disabled = false;
        sdkStopContinousRecognitionBtn.disabled = true;
      };

      // Signals that the speech service has started to detect speech.
      reco.speechStartDetected = function (s, e) {
        window.console.log(e);
        statusDiv.innerHTML += "(speechStartDetected) SessionId: " + e.sessionId + "\r\n";
      };

      // Signals that the speech service has detected that speech has stopped.
      reco.speechEndDetected = function (s, e) {
        window.console.log(e);
        statusDiv.innerHTML += "(speechEndDetected) SessionId: " + e.sessionId + "\r\n";
      };

      // Starts recognition
      reco.startContinuousRecognitionAsync();

      sdkStartContinousRecognitionBtn.disabled = true;
      sdkStopContinousRecognitionBtn.disabled = false;
    });

    // Stops recognition and disposes of resources.
    sdkStopContinousRecognitionBtn.addEventListener("click", function () {
      reco.stopContinuousRecognitionAsync(
        function () {
          reco.close();
          reco = undefined;
        },
        function (err) {
          reco.close();
          reco = undefined;
        });

      sdkStartContinousRecognitionBtn.disabled = false;
      sdkStopContinousRecognitionBtn.disabled = true;
    });







      // If an audio file was specified, use it. Else use the microphone.
      // Depending on browser security settings, the user may be prompted to allow microphone use. Using continuous recognition allows multiple
      // phrases to be recognized from a single use authorization.
      var audioConfig = SpeechSDK.AudioConfig.fromDefaultMicrophoneInput();
      var speechConfig;
      speechConfig = SpeechSDK.SpeechConfig.fromSubscription("f9bef20c5e064a65b33c3467e56f8e8b", "westus");

      speechConfig.speechRecognitionLanguage = "en-US";
      reco = new SpeechSDK.IntentRecognizer(speechConfig, audioConfig);

      // Set up a Language Understanding Model from Language Understanding Intelligent Service (LUIS).
      // See https://www.luis.ai/home for more information on LUIS.


      // Before beginning speech recognition, setup the callbacks to be invoked when an event occurs.

      // The event recognizing signals that an intermediate recognition result is received.
      // You will receive one or more recognizing events as a speech phrase is recognized, with each containing
      // more recognized speech. The event will contain the text for the recognition since the last phrase was recognized.
      reco.recognizing = function (s, e) {
        window.console.log(e);
        statusDiv.innerHTML += "(recognizing) Reason: " + SpeechSDK.ResultReason[e.result.reason] + " Text: " + e.result.text + "\r\n";
        phraseDiv.innerHTML = e.result.text;
      };

      // The event signals that the service has stopped processing speech.
      // https://docs.microsoft.com/javascript/api/microsoft-cognitiveservices-speech-sdk/speechrecognitioncanceledeventargs?view=azure-node-latest
      // This can happen for two broad classes or reasons.
      // 1. An error is encountered.
      //    In this case the .errorDetails property will contain a textual representation of the error.
      // 2. No additional audio is available.
      //    Caused by the input stream being closed or reaching the end of an audio file.
      reco.canceled = function (s, e) {
        window.console.log(e);

        statusDiv.innerHTML += "(cancel) Reason: " + SpeechSDK.CancellationReason[e.reason];
        if (e.reason === SpeechSDK.CancellationReason.Error) {
          statusDiv.innerHTML += ": " + e.errorDetails;
        }
        statusDiv.innerHTML += "\r\n";
      };

      // The event recognized signals that a final recognition result is received.
      // This is the final event that a phrase has been recognized.
      // For continuous recognition, you will get one recognized event for each phrase recognized.
      reco.recognized = function (s, e) {
        window.console.log(e);

        statusDiv.innerHTML += "(recognized)  Reason: " + SpeechSDK.ResultReason[e.result.reason];

        // Depending on what result reason is returned, different properties will be populated.
        switch (e.result.reason) {
          // This case occurs when speech was successfully recognized, but the speech did not match an intent from the Language Understanding Model.
          case SpeechSDK.ResultReason.RecognizedSpeech:
            statusDiv.innerHTML += " Text: " + e.result.text;
            break;

          // Both speech an intent from the model was recognized.
          case SpeechSDK.ResultReason.RecognizedIntent:
            statusDiv.innerHTML += " Text: " + e.result.text + " IntentId: " + e.result.intentId;

            // The actual JSON returned from Language Understanding is a bit more complex to get to, but it is available for things like
            // the entity name and type if part of the intent.
            statusDiv.innerHTML += " Intent JSON: " + e.result.properties.getProperty(SpeechSDK.PropertyId.LanguageUnderstandingServiceResponse_JsonResult);
            break;

          // No match was found.
          case SpeechSDK.ResultReason.NoMatch:
            var noMatchDetail = SpeechSDK.NoMatchDetails.fromResult(e.result);
            statusDiv.innerHTML += " NoMatchReason: " + SpeechSDK.NoMatchReason[noMatchDetail.reason];
            break;
        }
        statusDiv.innerHTML += "\r\n";
      };

      // Signals that a new session has started with the speech service
      reco.sessionStarted = function (s, e) {
        window.console.log(e);
        statusDiv.innerHTML += "(sessionStarted) SessionId: " + e.sessionId + "\r\n";
      };

      // Signals the end of a session with the speech service.
      reco.sessionStopped = function (s, e) {
        window.console.log(e);
        statusDiv.innerHTML += "(sessionStopped) SessionId: " + e.sessionId + "\r\n";
        sdkStartContinousRecognitionBtn.disabled = false;
        sdkStopContinousRecognitionBtn.disabled = true;
      };

      // Signals that the speech service has started to detect speech.
      reco.speechStartDetected = function (s, e) {
        window.console.log(e);
        statusDiv.innerHTML += "(speechStartDetected) SessionId: " + e.sessionId + "\r\n";
      };

      // Signals that the speech service has detected that speech has stopped.
      reco.speechEndDetected = function (s, e) {
        window.console.log(e);
        statusDiv.innerHTML += "(speechEndDetected) SessionId: " + e.sessionId + "\r\n";
      };

      // Note: this is how you can process the result directly
      //       rather then subscribing to the recognized
      //       event


    Initialize(function (speechSdk) {
      SpeechSDK = speechSdk;
    });
  });
</script>
</body>
</html>
